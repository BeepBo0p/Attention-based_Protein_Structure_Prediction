# Predicting Secondary Structure

In this project I have created a tool for predicting secondary protein structure by framing it as a classification task akin to the work by Ning Qian and Terry Sejnowski. I attempt to use the Transformer architecture and sequence-to-sequence framework to incorporate a larger context window than Qian and Sejnowski used, hoping that the attention-mechanism in the Transformer will allow for flexible usage of both local and global context within the sequence to create a more performant classifier. It did not really work.

## Project Structure

```
bioinfo_project_frfr/
├── .pre-commit-config.yaml                 # Pre-commit hooks configuration
├── .gitignore                              # Git ignore rules
├── .python-version                         # Python version used in the project. Generated by uv
├── pyproject.toml                          # Project dependencies and metadata
├── README.md                               # Project documentation
├── data/                                   # Directory for datasets and processed data
├── notebooks/                              # Jupyter-style Python scripts for exploration
├── src/                                    # Source code for the project
│   ├── pre_process_data.py                 # Pipeline for processing dataset into consumable format
│   └── protein_structure_prediction.py     # Main pipeline for protein structure prediction
└── out/                                    # Directory for model outputs and logs
```

---

## Setup Instructions

1. **Clone the Repository**
   ```bash
   git clone <repository-url>
   cd bioinfo_project_frfr
   ```

2. **Set Up Python Environment**
   Use `uv` to manage the Python environment:
   ```bash
   pip install uv   # Install uv using pip if you don't have it
   uv sync          # Setup the virtual environment
   ```

3. **(Opt.) Set Up Pre-commit Hooks**
   Install pre-commit hooks to ensure code quality if doing development:
   ```bash
   uvx pre-commit install
   ```

4. **Download Dataset**
   Download the dataset at the URL: https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Protein+Secondary+Structure)
   Unzip the dataset within the `data/` directory. Ensure the final directory structure is `data/Protein_Secondary_Structure/` and that the following files are present:
   - `protein-secondary-structure.train`
   - `protein-secondary-structure.test`

5. **Preprocess the dataset**
   Preprocess the dataset into csv files using the `pre_process_data.py` pipeline within the `src` folder by running the following commands:
   ```bash
   uv run src/pre_process_data.py
   ```
   Check that the pipeline is successful by looking for `protein-secondary-structure.train.csv` and `protein-secondary-structure.train.csv` within `data/Protein_Secondary_Structure/`

---

## Running the Token Prediction Pipeline

The `protein_structure_prediction` pipeline trains a model to predict the secondary structure of proteins.

1. **Run the Pipeline**
   Execute the `protein_structure_prediction.py` script:
   ```bash
   uv run src/protein_structure_prediction.py
   ```

2. **Output**
   The pipeline will generate the following outputs in the `out/<id>` directory under a monotonically increasing ID scheme:
   - `model.pth`: Trained model weights
   - `train_stats.csv`: Training statistics
   - `train_stats.png`: Training loss and metric plots
   - `predictions.txt`: Predictions on the test dataset

3. **Evaluate Results**
   Check the `predictions.txt` file for the average prediction accuracy and detailed predictions.

---

## Dataset Information

URL to dataset: https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Protein+Secondary+Structure)

### Additional Information

The dataset used was originally created by Ning Qian and Terry Sejnowski in their study using a neural net to predict the secondary structure of certain globular proteins. The idea is to take a linear sequence of amino acids and predict, for each amino acid, what secondary structure it is a part of within the protein. There are three choices: alpha-helix, beta-sheet, and random-coil.